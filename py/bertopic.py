# -*- coding: utf-8 -*-
"""BERTopic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dbM9UF-qh4-5bGXRDKKGkIdm2xQaRGL7

## BERTopic
**references**  

github: https://github.com/MaartenGr/BERTopic  
help: https://reposhub.com/python/natural-language-processing/MaartenGr-BERTopic.html

# **Installing BERTopic**

We start by installing BERTopic from PyPi:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install bertopic

!pip install sentence_transformers

import nltk
nltk.download("popular")

"""## Restart the Notebook
After installing BERTopic, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.

From the Menu:

Runtime → Restart Runtime

# Data
For this example, we use the popular 20 Newsgroups dataset which contains roughly 18000 newsgroups posts
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
dt = pd.read_csv('patent/DTx_patents_filtered_period_0722.csv')

docs = {'wipsonkey': [], 'docs': []}
docs['wipsonkey'] = dt['wipsonkey'].to_list()
docs['docs'] = [dt['발명의 명칭'][i] + '[SEP]' + dt['요약'][i] + '[SEP]' + dt['대표청구항'][i] for i in range(dt.shape[0])]
# docs['timestamp'] = dt['출원일'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d')).to_list()
docs['timestamp'] = dt['출원일'].apply(lambda x: int(x[:4])).to_list()

import pandas as pd
nondtx = pd.read_csv('patent/non_DTx_patents_filtered_period_0729.csv')

nondtx.shape

nondtx.info()

nondtx['대표청구항'] = nondtx['대표청구항'].fillna(' ')
target_docs = {'wipsonkey': [], 'docs': []}
target_docs['wipsonkey'] = nondtx['wipsonkey'].to_list()
target_docs['docs'] = [nondtx['발명의 명칭'][i] + '[SEP]' + nondtx['요약'][i] + '[SEP]' + nondtx['대표청구항'][i] for i in range(nondtx.shape[0])]
target_docs['docs'] = list(map(preprocessing, target_docs['docs']))
# target_docs['출원인'] = nondtx
target_docs['timestamp'] = nondtx['출원일'].apply(lambda x: int(x[:4])).to_list()

"""### preprocessing"""

# https://www.kaggle.com/tanulsingh077
def clean_text(text):
    import re

    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower() 
    text = re.sub('https?://\S+|www\.\S+', '', text) 
    text = re.sub('\n', '', text) 
    text = re.sub('[_=+,;#/\?:^$@*\"※~&ㆍ!│』\\‘|\<\>`\'…》“”【】]', '', text) 
    text = text.replace('-', ' ')
    text = re.sub('[0-9]+', '', text) 
    return text

def remove_stopwords(text):
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    return text

import nltk
stop_words = nltk.corpus.stopwords.words('english')
more_stopwords = []
stop_words = stop_words + more_stopwords

lemma = nltk.WordNetLemmatizer()

def lemma_text(text):
    text = ' '.join(lemma.lemmatize(word) for word in text.split(' '))
    return text

def preprocessing(text):
    text = clean_text(text)
    text = remove_stopwords(text)
    text = lemma_text(text)
    return text

docs['docs'] = list(map(preprocessing, docs['docs']))

target_docs['docs'] = list(map(preprocessing, target_docs['docs']))

"""# **Topic Modeling**

In this example, we will go through the main components of BERTopic and the steps necessary to create a strong topic model.

## Training

We start by instantiating BERTopic. We set language to `english` since our documents are in the English language. If you would like to use a multi-lingual model, please use `language="multilingual"` instead. 

We will also calculate the topic probabilities. However, this can slow down BERTopic significantly at large amounts of data (>100_000 documents). It is advised to turn this off if you want to speed up the model.
"""

from bertopic import BERTopic

topic_model = BERTopic(language="english", calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(docs['docs'])

from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

modelnm = 'paraphrase-mpnet-base-v2' # 'allenai-specter', 'nli-roberta-base-v2'

sentence_model = SentenceTransformer(modelnm)
topic_model = BERTopic(embedding_model=sentence_model, calculate_probabilities=True, verbose=True)

topics, probs = topic_model.fit_transform(docs['docs'])

"""**NOTE**: Use `language="multilingual"` to select a model that support 50+ languages."""

from bertopic import BERTopic

# Load model
topic_model = BERTopic.load("project/BERTopic/BERTopic_2107310737_paraphrase-mpnet-base-v2_title_claims_summary")

import pickle

with open('project/BERTopic/BERTopic_2107310737_paraphrase-mpnet-base-v2_topics_title_claims_summary.pickle', 'rb') as f:
    topics = pickle.load(f)

with open('project/BERTopic/BERTopic_2107310737_paraphrase-mpnet-base-v2_probs_title_claims_summary.pickle', 'rb') as f:
    probs = pickle.load(f)

topic_df = pd.DataFrame({'wipsonkey': dt['wipsonkey'], 'topic': topics})

"""## Extracting Topics
After fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents. 
"""

freq = topic_model.get_topic_info(); freq.head(10)

"""-1 refers to all outliers and should typically be ignored. Next, let's take a look at a frequent topic that were generated:"""

topic_model.get_topic(0)  # Select the most frequent topic

"""**NOTE**: BERTopic is stocastich which mmeans that the topics might differ across runs. This is mostly due to the stocastisch nature of UMAP.

# **Visualization**
There are several visualization options available in BERTopic, namely the visualization of topics, probabilities and topics over time. Topic modeling is, to a certain extent, quite subjective. Visualizations help understand the topics that were created.

## Visualize Topics
After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good 
understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. 
Instead, we can visualize the topics that were generated in a way very similar to 
[LDAvis](https://github.com/cpsievert/LDAvis):
"""

topic_model.visualize_topics()

"""## Visualize Topic Probabilities

The variable `probabilities` that is returned from `transform()` or `fit_transform()` can 
be used to understand how confident BERTopic is that certain topics can be found in a document. 

To visualize the distributions, we simply call:
"""

topic_model.visualize_distribution(probs[7], min_probability=0.015)

"""## Visualize Topic Hierarchy

The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. This might help selecting an appropriate nr_topics when reducing the number of topics that you have created.
"""

topic_model.visualize_hierarchy(top_n_topics=50)

"""## Visualize Terms

We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other.
"""

topic_model.visualize_barchart(top_n_topics=6)

"""## Visualize Topic Similarity
Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other.
"""

topic_model.visualize_heatmap(n_clusters=5, width=1000, height=1000)

"""## Topics over Time
Although extracting the topics and their representation is interesting, we are missing some dimensional information. For example, some topics might not be relevant anymore or some are gaining traction over the last years. That can be vital information when making decisions.

Here, we will model the topics over the years. For each topic and timestep, we calculate the c-TF-IDF representation. This will result in a specific topic representation at each timestep without the need to create clusters from embeddings as they were already created. However, topics can be regarded as evolutionary entities that evolve and change over time. This means that a topic representation at timestep t should be related to its representation at timesetps t-1 and t+1. To model this evolutionary trend, we average its c-TF-IDF representation with that of the c-TF-IDF representation at timestep t-1. This is done for each topic representation allowing for the representations to evolve over time.

(refer to https://maartengr.github.io/BERTopic/tutorial/topicsovertime/topicsovertime.html  
and https://www.kaggle.com/maartengr/topic-modeling-arxiv-abstract-with-bertopic)
"""

topics_over_time = topic_model.topics_over_time(docs['docs'], topics, docs['timestamp'], nr_bins=None, evolution_tuning=True)

topics_over_time['original'] = topics_over_time['Frequency']

topics_over_time['Frequency'] = topics_over_time.groupby(['Topic'])['original'].transform(lambda x: x.ewm(halflife=5).mean()); topics_over_time

topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=6, width=900, height=500)

alpha = 0.9
topics_over_time['Frequency'] = topics_over_time.groupby(['Topic'])['original'].transform(lambda x: x.ewm(alpha=alpha).mean()); topics_over_time

topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=6, width=900, height=500)

topics_over_time.groupby('Topic')[['Topic','Frequency']].tail(1).to_csv('topic_weight_{}.csv'.format(alpha), index=False)

"""# **Model serialization**
The model and its internal settings can easily be saved. Note that the documents and embeddings will not be saved. However, UMAP and HDBSCAN will be saved. 
"""

# Save model
import datetime
date = datetime.datetime.today().strftime('%y%m%d%H%M')
topic_model.save("project/BERTopic/BERTopic_{}_{}_title_claims_summary".format(date,model_nm))

import pickle

with open('project/BERTopic/BERTopic_{}_{}_topics_title_claims_summary.pickle'.format(date,model_nm)), 'wb') as f:
    pickle.dump(topics, f)

with open('project/BERTopic/BERTopic_{}_{}_probs_title_claims_summary.pickle'.format(date,model_nm)), 'wb') as f:
    pickle.dump(probs, f)