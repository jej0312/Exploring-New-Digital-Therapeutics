# -*- coding: utf-8 -*-
"""Recommending_nonDTx.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQI3Gyj1l1olKtU1BhJ4SrEJd6YLf9a9
"""

!pip install -U sentence_transformers

modelnm = 'AI-Growth/PatentSBERTa'

# from sentence_transformers import SentenceTransformer, util
# modelnm = 'allenai-specter' # 논문 데이터로 학습된 모델
# model = SentenceTransformer(modelnm)


# Load model from HuggingFace Hub
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer(modelnm)

# tokenizer = AutoTokenizer.from_pretrained(modelnm)
# model = AutoModel.from_pretrained(modelnm)

from google.colab import drive
drive.mount('/content/drive')

# cd /content/drive/MyDrive/01. ISL

cd /content/drive/Shareddrives/eunji/DTx

import torch
import numpy as np
import pandas as pd

data_group1 = pd.read_csv('patent/DTx_patents_filtered_period_0722.csv')

data_group2 = pd.read_csv('patent/non_DTx_patents_filtered_period_0729.csv')

"""# Preprocessing"""

# https://www.kaggle.com/tanulsingh077
def clean_text(text):
    import re

    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower() 
    text = re.sub('https?://\S+|www\.\S+', '', text) 
    text = re.sub('\n', '', text) 
    text = re.sub('[_=+,;#/\?:^$@*\"※~&ㆍ!│』\\‘|\<\>`\'…》“”【】]', '', text) 
    text = text.replace('-', ' ')
    text = re.sub('[0-9]+', '', text) 
    return text

def remove_stopwords(text):
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    return text

import nltk
nltk.download('popular')
stop_words = nltk.corpus.stopwords.words('english')
more_stopwords = []
stop_words = stop_words + more_stopwords

lemma = nltk.WordNetLemmatizer()

def lemma_text(text):
    text = ' '.join(lemma.lemmatize(word) for word in text.split(' '))
    return text

def preprocessing(text):
    text = clean_text(text)
    text = remove_stopwords(text)
    text = lemma_text(text)
    return text

data_group1['대표청구항'] = data_group1['대표청구항'].apply(lambda x: preprocessing(x))

data_group2['대표청구항'] = data_group2['대표청구항'].apply(lambda x: preprocessing(x))

"""# Technology Recommendation"""

topic_model = 'paraphrase-mpnet-base-v2'

import glob
import os

file_list = glob.glob(os.path.join("project/BERTopic", "*"+topic_model+"_topics*.pickle"))
# file_list = [file for file in file_list if file.split('\\')[-1] != 'A6_0_2400.csv']
file_list = sorted(file_list) # 0729 선택

print(file_list[0])

import pickle 
with open(file_list[0], 'rb') as f:
    dtx_topics = pickle.load(f)

len(dtx_topics) == data_group1.shape[0]

data_group1['topic'] = dtx_topics

del dtx_topics

file_list = glob.glob(os.path.join("patent", "non_DTx_patents_topic_result_*"+topic_model+"*.csv"))
# file_list = [file for file in file_list if file.split('\\')[-1] != 'A6_0_2400.csv']
file_list = sorted(file_list) # 0729 선택

print(file_list)

topic_info = pd.read_csv(file_list[0]);

topic_info.shape[0] == data_group2.shape[0]

data_group2['topic'] = topic_info['topic1']
data_group2['degree'] = topic_info['similarity1']

"""### sentence similarity"""

topic_count = {topic: count for topic, count in zip(data_group2['topic'].value_counts().index, data_group2['topic'].value_counts())}

topic_weight = pd.read_csv('topic_weight_0.3.csv');
topic_weight = {t:f for t, f in zip(topic_weight['Topic'], topic_weight['Frequency'])}

topic_weight

def calculate_score(model, data_groups, column, top_k='all'):
    '''
    weighted average
    '''

    from tqdm import tqdm

    data_group1, data_group2 = data_groups
    data_group1 = data_group1[['wipsonkey', '발명의 명칭', '대표청구항', '요약', '출원인', '출원일', 'topic']]
    data_group2 = data_group2[['wipsonkey', '발명의 명칭', '대표청구항', '요약', '출원인', '출원일', 'topic', 'degree']]

    idx1 = {idx: value for idx, value in enumerate(data_group1['topic'])}
    idx2 = {idx: value for idx, value in enumerate(data_group2['topic'])}
    
    #Compute embeddings
    print("Computing embeddings")
    embeddings_group1 = model.encode(data_group1[column], convert_to_tensor=True)
    embeddings_group2 = model.encode(data_group2[column], convert_to_tensor=True)

    print("\nComputing cosine score")
    cosine_scores = util.pytorch_cos_sim(embeddings_group1, embeddings_group2)
    # print(cosine_scores.shape)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    data = data_group1['topic'].apply(lambda x: list(weight[x].values()))

    print("Computing topic score")
    for i in tqdm(range(len(data))): #### CASE 2 ####
      top_w = data_group1['topic'].apply(lambda x: topic_weight[x])[i]
      # data[i] = [(val * top_w) for val in data[i]]
      top_c = data_group1['topic'].apply(lambda x: topic_count[x])[i]
      data[i] = [(val * top_w / top_c) for val in data[i]]
 
    data = pd.DataFrame(data).topic.apply(pd.Series)
    # print(data.shape)
    data = cosine_scores * torch.tensor(data.values, device=device)
 
    print("Computing final score")
    # result = torch.div(torch.sum(data,axis=0), data_group1.shape[0]) #### CASE 3 ####
    result = torch.sum(data,axis=0) #### CASE 1 ####

    if top_k != 'all':
        values, indexs = torch.topk(result, k=top_k, dim=-1) # top k개만큼 추출
        top_df = pd.concat([data_group2.iloc[indexs.tolist(), :].reset_index(drop=True), pd.Series(result.to(torch.device("cpu")).numpy()).iloc[indexs.tolist()].reset_index(drop=True)], axis=1, ignore_index=True)

    else:
        top_df = pd.concat([data_group2, pd.Series(result.to(torch.device("cpu")).numpy())], axis=1, ignore_index=True)

    top_df.columns = ['wipsonkey', '발명의 명칭', '대표청구항', '요약', '출원인', '출원일', 'topic', 'topic_degree', 'score']

    top_df = top_df[['wipsonkey', '발명의 명칭', '대표청구항', '요약', '출원인', 'topic', 'score']]
    top_df['topic'] = list(map(int, top_df['topic']))
    top_df['wipsonkey'] = list(map(int, top_df['wipsonkey']))
    
    return embeddings_group1, embeddings_group2, cosine_scores, top_df

embeddings_group1, embeddings_group2, cosine_scores, results = calculate_score(model, [data_group1, data_group2], '대표청구항')

import matplotlib.pyplot as plt
import seaborn as sns

sns.distplot(cosine_scores.to(torch.device("cpu")).numpy().ravel())

pd.DataFrame(cosine_scores.to(torch.device("cpu")).numpy().ravel()).describe().apply(lambda x: round(x,3))

np.quantile(cosine_scores.to(torch.device("cpu")).numpy().ravel(), [0.99, 0.9, 0.8])

np.var(cosine_scores.to(torch.device("cpu")).numpy().ravel())

"""- case 

```python
    print("Computing topic score")
    for i in tqdm(range(len(data))): 
      top_w = data_group1['topic'].apply(lambda x: topic_weight[x])[i]
      # data[i] = [(val * top_w) for val in data[i]]
      top_c = data_group1['topic'].apply(lambda x: topic_count[x])[i]
      data[i] = [(val * top_w / top_c) for val in data[i]]
    
    data = pd.DataFrame(data).topic.apply(pd.Series)
    # print(data.shape)
    data = cosine_scores * torch.tensor(data.values, device=device)
    
    print("Computing final score")
    # result = torch.div(torch.sum(data,axis=0), data_group1.shape[0]) 
    result = torch.sum(data,axis=0) 
```
"""

"""
59  -1_user_least_data_system -- 1258
37	0_dioxide_carbon_breathing_gas -- 18
37	1_cognitive_stimulus_training_performance -- 160
29	2_sensor_device_monitoring_signal -- 2062
27	3_patient_input_device_interface -- 5126
16	4_movement_motion_measure_tremor -- 32 
"""

#### alpha 0.3
results.sort_values('score', ascending=False)[:10]

results.groupby('topic')['score'].describe()

"""
59  -1_user_least_data_system -- 1258
37	0_dioxide_carbon_breathing_gas -- 18
37	1_cognitive_stimulus_training_performance -- 160
29	2_sensor_device_monitoring_signal -- 2062
27	3_patient_input_device_interface -- 5126
16	4_movement_motion_measure_tremor -- 32 
"""

#### alpha 0.5 
results.sort_values('score', ascending=False)[:10]

results.sort_values('score', ascending=True)[:10]

results.sort_values('score', ascending=False)[:20]

results.groupby('topic')['score'].describe()

"""
59  -1_user_least_data_system -- 1258
37	0_dioxide_carbon_breathing_gas -- 18
37	1_cognitive_stimulus_training_performance -- 160
29	2_sensor_device_monitoring_signal -- 2062
27	3_patient_input_device_interface -- 5126
16	4_movement_motion_measure_tremor -- 32 
"""

#### alpha 0.9
results.sort_values('score', ascending=False)[:10]

results.groupby('topic')['score'].describe()

import matplotlib.pyplot as plt
import seaborn as sns

"""
59  -1_user_least_data_system -- 1258
37	0_dioxide_carbon_breathing_gas -- 18
37	1_cognitive_stimulus_training_performance -- 160
29	2_sensor_device_monitoring_signal -- 2062
27	3_patient_input_device_interface -- 5126
16	4_movement_motion_measure_tremor -- 32 
"""

grp_col_dict = {-1: 'gray',
                0: 'orange', 
                1: 'skyblue',
                2: 'green',
                3: 'brown', 
                4: 'yellow'
                # , 5: 'purple'
                }

plt.figure(figsize=(10,6))

# for loop of species group
for group in grp_col_dict:
    
    # subset of group
    subset = results[results['topic'] == group]
    
    # histogram and kernel density curve
    sns.distplot(subset['score'], 
                hist = False, # histogram
                kde = True,  # density curve
                kde_kws = {'linewidth': 2}, 
                color = grp_col_dict[group],
                label = group)
# setting plot format
plt.title('Density Plot by Topics')
plt.xlabel('Potential Score')
plt.ylabel('Density')
plt.legend(prop={'size': 10}, title = 'Topic')
plt.show()

import pickle

with open('patent/embeddings_group1_{}+{}_preprocessed.pickle'.format(topic_model, modelnm.replace('/', '_')), 'wb') as f:
    pickle.dump(embeddings_group1, f)

with open('patent/embeddings_group2_{}+{}_preprocessed.pickle'.format(topic_model, modelnm.replace('/', '_')), 'wb') as f:
    pickle.dump(embeddings_group2, f)

with open('patent/cosine_similarity_{}+{}_preprocessed.pickle'.format(topic_model, modelnm.replace('/', '_')), 'wb') as f:
    pickle.dump(cosine_scores, f)

with open('patent/sentence_similarity_weighted_average_{}+{}_preprocessed_case3.pickle'.format(topic_model, modelnm.replace('/', '_')), 'wb') as f:
    pickle.dump(results, f)

"""# Target disorder recommendation

- pair ranking
"""

import pickle

with open('patent/cosine_similarity_{}+{}_preprocessed.pickle'.format(topic_model, modelnm.replace('/', '_')), 'rb') as f:
    cosine_scores = pickle.load(f)

with open('patent/sentence_similarity_weighted_average_{}+{}_preprocessed_case3.pickle'.format(topic_model, modelnm.replace('/', '_')), 'rb') as f:
    results = pickle.load(f)

label = pd.read_csv('disease_label_all_210727.csv')

import ast

label.CUI_list = label.CUI_list.apply(lambda x: ast.literal_eval(x)) # str으로 작성된 list를 list 형식으로
label_flattened = label.explode('CUI_list')

label_flattened = label_flattened[~label_flattened[['CUI_list', 'wipsonkey']].duplicated()]

import pickle
# with open('mental_and_behavioral_CUIs_final_0726.pickle', 'rb') as f:
with open('psychiatric_disorder_CUIs_0724.pickle', 'rb') as f:
    filt = pickle.load(f)

filt = {filt['cui'][k]: filt['name'][k] for k in range(len(filt['cui']))}

label_flattened = label_flattened[label_flattened['CUI_list'].apply(lambda x: x in filt.keys())]

label_flattened['disease_name'] = label_flattened['CUI_list'].apply(lambda x: filt[x] if x in filt.keys() else x)

"""- target info"""

def find_targets_for_nonDTx(wipsonkey, similarity_threshold = 0.85):
    import pprint

    print("nonDTx:", wipsonkey, "(", data_group2[data_group2['wipsonkey'] == wipsonkey]['발명의 명칭'].values[0], ")\n")

    if wipsonkey in label_flattened['wipsonkey']:
        result = label_flattened[label_flattened['wipsonkey'] == wipsonkey].groupby('disease_name')['disease_name'].count().to_dict()
        print("\nDiseases from the patents are:")
        
    else:
        idx2 = data_group2['wipsonkey'].apply(lambda x: x == wipsonkey) # target을 찾고자 하는 nonDTx
        # Find the pairs with the highest cosine similarity scores
        pairs = []
        for i in range(cosine_scores.shape[0]): # i: DTx
            # for j in range(cosine_scores.shape[1]):
            j = idx2[idx2].index[0] # j: nonDTx
            pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})

        # Sort scores in decreasing order
        pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)

        rows = []
        for pair in pairs:
            i, j = pair['index']
            if j == idx2[idx2].index[0]:
                rows.append([data_group1['wipsonkey'].iloc[i], data_group2['wipsonkey'].iloc[j], pair['score'].item()])

        wipsonkey_list = [item[0] for item in rows if item[-1] > similarity_threshold]
        print("This patent is similar to:")
        for wips in wipsonkey_list:
          print(wips, "(", data_group1[data_group1['wipsonkey'] == wips]['발명의 명칭'].values[0], ")")
        result = label_flattened[label_flattened['wipsonkey'].apply(lambda x: x in wipsonkey_list)].groupby('disease_name')['disease_name'].count().to_dict()
        print("\nDiseases from similar DTx patents are:")
    
    result = sorted(result.items(), reverse=True, key=lambda item: item[1])
    pprint.pprint(result)

    return result

"""Extracting 5 Recommended Diseases"""

result = find_targets_for_nonDTx(4919030000771, 0.785)

result = find_targets_for_nonDTx(5419010000387, 0.81)

result = find_targets_for_nonDTx(5419021006632, 0.779)

result = find_targets_for_nonDTx(4919016000973, 0.775)

result = find_targets_for_nonDTx(4919033001057, 0.8)